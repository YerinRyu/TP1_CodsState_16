{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuriCha02/TP1_CodsState_16/blob/main/%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98_%EC%A0%84%EC%B2%B4%EC%A0%81%EC%BD%94%EB%93%9C_%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파라미터 설정"
      ],
      "metadata": {
        "id": "ESSIQFqsA5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# 정규분포를 갖는 난수값을 생성하기 위한 매개변수 정의\n",
        "RND_MEAN, RND_STD = 0.0, 1.0\n",
        "LEARNING_RATE = 0.01"
      ],
      "metadata": {
        "id": "mMSYQW5gA8vA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level1: multiple_main()"
      ],
      "metadata": {
        "id": "Efj526P2u6aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://codestates.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F53c2ac3f-6444-45e8-a72e-0926b15f5a1d%2FUntitled.png?table=block&id=11d2da23-b3bc-4859-94bc-97eb1c51069c&spaceId=82d63a72-8254-4cde-bf1e-b2597b7c099c&width=2000&userId=&cache=v2)"
      ],
      "metadata": {
        "id": "Aol1xxA81heX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "AFzL-PCdJU8-"
      },
      "outputs": [],
      "source": [
        "#%run /content/AnnModel1.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`AnnModel1.ipynb`파일내부에 method를 사용할수 있게, 실행함"
      ],
      "metadata": {
        "id": "h2Siw3p0F5SM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2Koe2owaJcNY"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def multiple_main(epoch_count = 10, mb_size = 10, report=1, train_ratio = 0.6):\n",
        "    multiple_load_dataset()\n",
        "    init_param()\n",
        "    train_and_test(epoch_count,mb_size,report,train_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "- `ANNModel1.ipynb`에서 `init_param()`과 `train_and_test()`을 불러옴\n",
        "- `multiple_load_dataset()`로 데이터를 불러와,\n",
        " - `data`, `input_cnt`, `output_cnt` 글로벌 변수를 출력함\n",
        "  - `input_cnt`, `output_cnt`는 AnnModel1에서 가져온 메서드 `init_param()`에 사용됨"
      ],
      "metadata": {
        "id": "OqRfToc0Fclc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level2: multiple_load_dataset()"
      ],
      "metadata": {
        "id": "6Q23-QO-u90-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "msTkqcMMJeoP"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def multiple_load_dataset():\n",
        "    with open('/content/mulit_classification_data.csv') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "    global data, input_cnt, output_cnt\n",
        "\n",
        "    input_cnt, output_cnt = 27, 7\n",
        "    data = np.asarray(rows, dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**Multiple_load_dataset()에 결과 값**\n",
        "\n",
        "data, input_cnt, output_cnt를 글로벌 변수값으로 저장해서, 다른 변수에서 사용이 가능함.\n",
        "- 대신 변수 이름이 겹칠 가능성이 높음.\n",
        "  - 특히 고객들이 코드를 편집해서 사용해야 할 경우\n",
        "\n",
        "`data`:\n",
        "\n",
        "- 행:\n",
        "> ```\n",
        "[ 4.20000e+01  5.00000e+01  2.70900e+05  2.70944e+05  2.67000e+02\n",
        "  1.70000e+01  4.40000e+01  2.42200e+04  7.60000e+01  1.08000e+02\n",
        "  1.68700e+03  1.00000e+00  0.00000e+00  8.00000e+01  4.98000e-02\n",
        "  2.41500e-01  1.81800e-01  4.70000e-03  4.70600e-01  1.00000e+00\n",
        "  1.00000e+00  2.42650e+00  9.03100e-01  1.64350e+00  8.18200e-01\n",
        " -2.91300e-01  5.82200e-01  1.00000e+00  0.00000e+00  0.00000e+00\n",
        "  0.00000e+00  0.00000e+00  0.00000e+00  0.00000e+00]\n",
        "```\n",
        "- 열 길이: 34\n",
        "- 행 길이: 1941"
      ],
      "metadata": {
        "id": "t6llJCIq7DYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level2: Init_param()"
      ],
      "metadata": {
        "id": "1iV6UsPQvMU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def init_param():\n",
        "\n",
        "    # 추후 다른 메서드에서 편하게 활용하기 위해 전역변수로 정의\n",
        "    global weight, bias\n",
        "\n",
        "    # 가중치 행렬 값들을 np.random.normal() 함수를 이용해 정규분포를 갖는 난수값으로 초기화 합니다.\n",
        "    # 행과 열은 [10,1] 로 맞춰 주며, 평균값 0.0, 표준편차 1.0의 정규분포를 따르도록 합니다.\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD, [input_cnt, output_cnt])\n",
        "\n",
        "    # 편향의 경우 하나의 값을 갖는 스칼라로 np.zeros() 메서드를 통해 0의 값을 갖도록 합니다.\n",
        "    # 편향은 초기에 지나친 영향을 주어 학습에 역효과를 불러오지 않도록 0으로로 초기화\n",
        "    bias = np.zeros([output_cnt])"
      ],
      "metadata": {
        "id": "nrAIYxN7hw4-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level2: train_and_test()"
      ],
      "metadata": {
        "id": "Bcz6_YXzwe2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def train_and_test(epoch_count, mb_size, report, train_ratio):\n",
        "\n",
        "    '''1) 데이터 셔플링 - arrange_data()'''\n",
        "    # 메서드를 통해 데이터를 섞어주고 1 에폭에 필요한 미니배치 수 를 반환합니다.\n",
        "    mini_batch_step_count = arrange_data(mb_size,train_ratio)\n",
        "\n",
        "    '''2) 테스트 데이터 분리 - get_test_data()'''\n",
        "    # 학습 데이터는 미니배치를 고려하여야 하기에\n",
        "    # 미니배치 고려가 필요 없는 테스트 데이터 분리 메서드를 먼저 배치합니다.\n",
        "    test_x, test_y = get_test_data()\n",
        "\n",
        "    # 다음은 학습 및 테스트 수행 단계를 정의하는 과정 입니다.\n",
        "    # 학습은 사용자가 지정한 학습횟수(에폭-epoch_count)에 맞춰 학습을 반복하도록 정의합니다.\n",
        "    for epoch in range(epoch_count):\n",
        "\n",
        "        # 학습 진행에 따른 정보를 담기 위해 빈 리스트를 사전에 정의하여 줍니다.\n",
        "        losses, accs = [], []\n",
        "\n",
        "        # 1 에폭이 수행되기 위해서는 미니배치 처리에 따른 학습이 모두 완료되어야 합니다.\n",
        "        # 그리고 에폭은 사용자가 1 이상의 값을 설정할 수 있어야 하므로,\n",
        "        # 이를 위한 이중 반복문 구조를 설계할 수 있습니다.\n",
        "        # 만약 학습 데이터의 수가 16개, 미니배치 크기(mb_size)가 4개라면\n",
        "        # 1 에폭을 위해서는 총 4번(mini_batch_step_count)의 미니배치에 따른 학습이 수행되어야 합니다.\n",
        "        for nth in range(mini_batch_step_count):\n",
        "\n",
        "            '''3) 학습 데이터 분리 - get_train_data()'''\n",
        "            # 학습 데이터의 경우 에폭마다의 미니배치 처리 단계(nth)를 고려하여 학습 데이터의 입출력 벡터를 변수화 합니다.\n",
        "            train_x, train_y = get_train_data(mb_size, nth)\n",
        "\n",
        "            '''4) 학습 - run_train()'''\n",
        "            # 미니배치 처리 단계(nth)에 따른 학습 데이터의 입출력 벡터를 학습합니다.\n",
        "            # 지금은 run_train() 메서드의 이름만 정의해둔 상황입니다.\n",
        "            # 정리하면 미니배치 단계에 따라 임의로 설정한 고정된 실험 결과값을 반환합니다.\n",
        "            #TODO run_train() 메서드는 다음 과정에서 하위 메서드 정의\n",
        "            loss, acc = run_train(train_x,train_y)\n",
        "\n",
        "            # 미니배치 단계에 따른 실험 결괏값(loss, acc)들을 append() 메서드로 묶어줍니다.\n",
        "            losses.append(loss)\n",
        "            accs.append(acc)\n",
        "\n",
        "        '''5) 테스트 - run_test()'''\n",
        "        # 테스트 과정은 사용자가 지정한 1 이상의 결과 보고 주기(report)에 따른 에폭 마다 수행하도록 합니다.\n",
        "        # 모든 에폭마다 테스트를 수행할 수도 있지만, 그건 효율적인 방식이 되지 못합니다.\n",
        "\n",
        "        # 상황 예제 1)\n",
        "        # 만약 학습 반복 주기(에폭)를 10 이라 하며, 결과 보고 주기(report)를 2 라 하면\n",
        "        # 다음과 같은 주기로 테스트 결과를 반환하도록 합니다.\n",
        "        # 0(x), 1(o), 2(x), 3(o), 4(x), 5(o), 6(x), 7(o), 8(x), 9(o)\n",
        "\n",
        "        # 상황 예제 2)\n",
        "        # 만약 학습 반복 주기(에폭)를 10 이라 하며, 결과 보고 주기(report)를 3 이라 하면\n",
        "        # 다음과 같은 주기로 테스트 결과를 반환하도록 합니다.\n",
        "        # 0(x), 1(x), 2(o), 3(x), 4(x), 5(o), 6(x), 7(x), 8(o), 9(x)\n",
        "\n",
        "        # 이와같은 조건을 갖는 주기를 반환할 수 있도록 코드를 작성합니다.\n",
        "\n",
        "        # 이러한 경우 나머지를 반환하는 연산자와 비교 연산자를 활용할 수 있습니다.\n",
        "        if report > 0 and (epoch+1) % report == 0:\n",
        "\n",
        "            # 위 조건에 부합하는 경우 테스트를 진행합니다.\n",
        "            acc = run_test(test_x, test_y)\n",
        "\n",
        "            # 수행한 결과를 사용자에게 출력합니다.\n",
        "            # 현재 에폭\n",
        "            # 현재 에폭에 따른 미니배치 단계별 결과의 학습 손실(loss) 평균값\n",
        "            # 현재 에폭에 따른 미니배치 단계별 결과의 학습 정확도(accs) 평균값\n",
        "            # 현재 에폭의 정확도(loss)\n",
        "            print(\"Epoch {}   : Train - Loss = {:.3f}, Accuracy = {:.3f} / Test - Accuracy = {:.3f}\".\\\n",
        "                  format(epoch+1, np.mean(losses), np.mean(accs), acc))\n",
        "\n",
        "\n",
        "    '''5) 최종 테스트 - run_test()'''\n",
        "    # 학습을 모두 마쳤다면 주어진 조건에 다른 모델 파라미터에 대한 조정이 완료되었다는 의미 입니다.\n",
        "    # 그렇기에 최종 테스트를 수행하여 학습을 모두 마친 AI 모델에 대한 성능 평가를 run_test() 메서드로 수행합니다.\n",
        "    final_acc = run_test(test_x, test_y)\n",
        "\n",
        "    # 학습에 따른 최종 결괏값 출력\n",
        "    print(\"=\"*30, ' Final TEST ', '='*30)\n",
        "    print('\\nFinal Accuracy = {:.3f}'.format(final_acc))"
      ],
      "metadata": {
        "id": "-hzKRiZOtQlt"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: arrange_data()"
      ],
      "metadata": {
        "id": "IJmb6b3ix5iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def arrange_data(mb_size, train_ratio):\n",
        "\n",
        "    # 활용 빈도가 높은 변수들은 전역변수로 할당하며, 활용빈도가 낮은 변수의 경우 반환처리 하였습니다.\n",
        "    global data, shuffle_map, test_begin_index\n",
        "\n",
        "    '''세부 기능 1)전체 데이터의 인덱싱 생성 후 셔플링.'''\n",
        "    # 실험에 쓰일 데이터(data)의 수에 맞게 인덱싱(0,1,2,3,...)을 생성하는 과정 입니다.\n",
        "    # 입출력 예시\n",
        "    # >>>data.shape[0]\n",
        "    # 20\n",
        "    shuffle_map = np.arange(data.shape[0])\n",
        "    # 학습의 효율을 높여주기 위해 인덱싱(shuffle_map)을 뒤섞어 줍니다.\n",
        "    np.random.shuffle(shuffle_map)\n",
        "\n",
        "    '''세부 기능 2)1에폭에 필요한 미니배치의 수 연산.'''\n",
        "    # 1 에폭을 위한 미니배치의 수(mini_batch_step_count)는\n",
        "    # 전체 데이터(data.shape[0])에서 학습 데이터의 비율(train_ratio)만큼의 개수를 구한 후\n",
        "    # 미니배치 크기(mb_size)로 나눠 몫 만을 구해줍니다.\n",
        "    # ( ※ mb_size 는 하나의 미니배치에 포함된 데이터의 수 입니다.)\n",
        "    mini_batch_step_count = int(data.shape[0] * train_ratio) // mb_size\n",
        "\n",
        "    '''세부 기능 3)학습 데이터와 테스트 데이터의 경계 인덱스를 구함.'''\n",
        "    # 전체 데이터에서 학습 데이터와 테스트 데이터가 나뉘는 경계가 되는 인덱스를\n",
        "    # 테스트 데이터가 시작되는 인덱스로 설정하였습니다.\n",
        "    # 이 값을 구하기 위해서는 다양한 방식이 있지만,\n",
        "    # 미니배치 스탭 수(mini_batch_step_count)와 미니배치 크기(mb_size)를 곱해\n",
        "    # 학습 및 테스트 데이터 경계 인덱스(test_begin_index)를 구해줍니다.\n",
        "    test_begin_index = mini_batch_step_count * mb_size\n",
        "\n",
        "    # 다수의 변수가 생성되었지만 mini_batch_step_count 변수만 밖으로 반환합니다.\n",
        "    # 활용 빈도가 높은 변수들은 전역변수로 할당하며, 활용빈도가 낮은 변수의 경우 반환처리 하였습니다.\n",
        "    return mini_batch_step_count"
      ],
      "metadata": {
        "id": "j0VA_ziRsrsY"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: get_train_data()"
      ],
      "metadata": {
        "id": "4vKqXzInx8h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def get_train_data(mb_size, nth):\n",
        "\n",
        "    # 학습 데이터의 경우 새로운 Epoch 의 학습이 수행되기 전(if nth == 0)\n",
        "    # 학습 데이터 인덱싱 정보를 무작위로 섞어(np.random.shuffle()) 학습의 효과를 높여줍니다.\n",
        "    # 학습의 효과를 높여주는 선행 연구에 근거함\n",
        "    if nth == 0:\n",
        "        np.random.shuffle(shuffle_map[:test_begin_index])\n",
        "\n",
        "    # 학습 데이터의 분리는 미니배치를 고려해야 합니다.\n",
        "    # 전체 데이터(data)에서 뒤섞인 인덱싱값으로 접근(shuffle_map)하여 미니배치 처리를 수행합니다.\n",
        "    # 만약 미니배치 사이즈(mb_size) 값이 4 라면 다음과 같이 나눠볼 수 있습니다.\n",
        "    # 0 : 4\n",
        "    # 4 : 8\n",
        "    # 8 : 12\n",
        "    # 12 : 16\n",
        "    # 그리고 이와 같은 배수의 연산을 위한 수식을 미니배치 크기(mb_size)와 몇 번째 미니배치 처리 단계(nth) 값을 활용합니다.\n",
        "    train_data = data[shuffle_map[mb_size * nth : mb_size * (nth+1)]]\n",
        "\n",
        "    # 미니배치에 따라 나눠진 학습 데이터를 입출력 벡터로 나눠 반환합니다.\n",
        "    return train_data[:,:-output_cnt], train_data[:,-output_cnt:]"
      ],
      "metadata": {
        "id": "W4DZvm3PlMDo"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: get_test_data()"
      ],
      "metadata": {
        "id": "nSuSAKzeyQ0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def get_test_data():\n",
        "\n",
        "    \"\"\" 전체 데이터에서 테스트 데이터 분리 과정 \"\"\"\n",
        "    # 전체 데이터(data)의 인덱싱 정보를 갖고 있는 변수(shuffle_map)에 접근하여 테스트 데이터를 얻어냅니다.\n",
        "    # 테스트 데이터의 경우 테스트 데이터의 시작 인덱싱 위치(test_begin_index)를 기준으로 인덱싱 끝 까지 범위를 갖도록 합니다.\n",
        "    test_data = data[shuffle_map[test_begin_index:]]\n",
        "\n",
        "    \"\"\" 테스트 데이터에서 입출력 벡터 분리 과정 \"\"\"\n",
        "    # test_data 변수는 입출력 벡터를 포함한 테스트 데이터 입니다.\n",
        "    # 그렇기에 입출력 벡터를 분리하기 위한 과정을 수행하여야 합니다.\n",
        "    # 출력 계층 값을 담는 변수(output_cnt)를 활용하여 슬라이싱 과정을 수행합니다.\n",
        "    # 행은 모두 포함하지만 열의 경우 다음과 같이 구분할 수 있습니다.\n",
        "    return test_data[:,:-output_cnt], test_data[:,-output_cnt:]"
      ],
      "metadata": {
        "id": "EJIqQl28svxX"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: run_train()"
      ],
      "metadata": {
        "id": "T_W39BzZyXKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def run_train(x, y):\n",
        "    # 1) 가장 먼저 순전파 연산이 수행되어야 합니다. 그렇기에 해당 메서드를 배치합니다.\n",
        "    # auxiliary(보조하다) 의 약자로 변수명에 aux 를 활용합니다.\n",
        "    # neuralnet의 약자 nn 을 활용합니다.\n",
        "    y_hat, aux_nn_x = forward_neuralnet(x)\n",
        "\n",
        "    # 2) 순전파의 결괏값(예측값)을 받아 AI 모델의 손실값을 확인합니다.\n",
        "    # postproc의 약자 pp 를 활용합니다.\n",
        "    loss, aux_pp_diff = forward_postproc(y_hat, y)\n",
        "\n",
        "    # 3) 예측값과 실제값을 활용하여 AI 모델의 정확도를 확인합니다.\n",
        "    accuracy = eval_accuracy(y_hat, y)\n",
        "\n",
        "    # 4) 순전파에 따른 여러 중간 지표를 확인하였으니 역전파를 수행하여 파라미터를 수정합니다.\n",
        "    # 다만 역전파에 앞서 손실함수의 미분 G_output(G) 를 구하여 줍니다.\n",
        "    G_output = backprop_postproc(aux_pp_diff)\n",
        "\n",
        "    # 5) G_output을 구하였으니, 실제 파라미터가 업데이트 되는 메서드를 배치합니다.\n",
        "    # 해당 메서드는 위의 다른 메서드와 달리 반환되어지는 값이 없습니다.\n",
        "    # 파라미터의 업데이트 후 전역변수를 취하였기에, 다음 순전파에서는 업데이트된 파라미터를 활용할 수 있습니다.\n",
        "    backprop_neuralnet(G_output, aux_nn_x)\n",
        "\n",
        "    # 위에서 구한 손실값과 정확도는 반환시켜 사용자에게 보여줄 수 있도록 취합니다.\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "Q-U7ykQXyk9N"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: forward_neuralnet()"
      ],
      "metadata": {
        "id": "tQozUXn3zDHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def forward_neuralnet(x):\n",
        "\n",
        "    y_hat = np.matmul(x, weight) + bias\n",
        "\n",
        "    return y_hat, x"
      ],
      "metadata": {
        "id": "0RrZHPl-zG0t"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: forward_postproc()"
      ],
      "metadata": {
        "id": "aeg9Wk491Utp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "PBzu6Z5DJrug"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def forward_postproc(y_hat, y_real):\n",
        "    entropy  = softmax_cross_entropy(y_real, y_hat)\n",
        "    loss     = np.mean(entropy)\n",
        "\n",
        "    return loss, [y_real, y_hat, entropy]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`entropy  = softmax_cross_entropy(y_real, y_hat)`: 예측값과 테스트 값을 비교하여 cross_entropy loss 값을 구하여 `entropy`로 저장함\n",
        "\n",
        "`loss = np.mean(entropy)`: `entropy`의 평균 값을 구함\n",
        "\n",
        "`return loss, [y_real, y_hat, entropy]`:\n",
        "- 평균 loss 점수을 출력함\n",
        "- 실제값과 예측값, 그리고 cross-entropy loss 값을 리스트로 출력함"
      ],
      "metadata": {
        "id": "psm9iw_CDiKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Level5: softmax_cross_entropy()"
      ],
      "metadata": {
        "id": "w6OTKBXX2IAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "vCuGFYtWJj8K"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax_cross_entropy(y_real, y_hat):\n",
        "\n",
        "    probs = softmax(y_hat)\n",
        "\n",
        "    #기존 확률값에 아주 작은 양수를 더해 log함수의 폭주 방지\n",
        "    return - np.sum(y_real * np.log(probs+1.0e-10), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`y_real`: test 확률\n",
        "\n",
        "`y_hat`: 예측확률\n",
        "\n",
        "\n",
        "`probs = softmax(y_hat)`예측확률을 softmax 확률로 변경하여 `probs`에 저장함\n",
        "\n",
        "`return - np.sum(y_real * np.log(probs+1.0e-10), axis=1)`: 각 행 (or 배치)에서 cross-entropy loss 값을 구함"
      ],
      "metadata": {
        "id": "ZL9FqWrjBXR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Level6: softmax()"
      ],
      "metadata": {
        "id": "2DeyHp0U2fjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "BkQXypCdJoQc"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax(x):\n",
        "\n",
        "    max_elem = np.max(x, axis = 1)\n",
        "\n",
        "    diff = (x.transpose() - max_elem).transpose()\n",
        "    exp = np.exp(diff)\n",
        "\n",
        "    sum_exp = np.sum(exp, axis = 1)\n",
        "\n",
        "    probs = (exp.transpose()/sum_exp).transpose()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**코드 설명:**\n",
        "\n",
        "`max_elem = np.max(x, axis = 1)`: 각 행에서 최대 값을 구하여, `max_elem`에 저장함\n",
        "\n",
        "`diff = (x.transpose() - max_elem).transpose()`: 각 행의 모든 값과 그 행의 차이를 구하여 `diff`에 저장함.\n",
        "- 아마 다름 `np.exp()`에서 negative 값을 positive 값으로 바꿀떄,값들 사이에 차이가 너무 벌어지지 않도록 차이를 구하서 거리를 좁히는 것 같음\n",
        "\n",
        "`exp = np.exp(diff)`: negative 값을 positve값으로 변경하여 `exp`로 저장함\n",
        "\n",
        "`sum_exp = np.sum(exp, axis = 1)`: 각 행에서 값들을 모두 더해서 `sum_exp`에 저장함\n",
        "\n",
        "`probs = (exp.transpose()/sum_exp).transpose()`: `exp`에서 'sum_exp'를 나누어 확률을 구함\n",
        "\n",
        "`return probs`: 확률을 function 외부로 배출함\n",
        "\n",
        "**추가**: pytorch를 사용할 경우, `transpose()`를 일일히 사용할 필요 없이, 내부에서 알아서 해줌."
      ],
      "metadata": {
        "id": "gOXdybx-8hOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: eval_accuracy()"
      ],
      "metadata": {
        "id": "ZdSrIlad13-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Ty-DSGNNJ2kO"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def eval_accuracy(y_hat,y_real):\n",
        "    estimate = np.argmax(y_hat,axis=1)\n",
        "    answer = np.argmax(y_real,axis=1)\n",
        "\n",
        "    correct = np.equal(estimate, answer)\n",
        "\n",
        "    return np.mean(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@ 차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`estimate = np.argmax(y_hat,axis=1)`: 예측확률 중에서 가장 가능성이 높은 레벨을 선택하여 `estimate`라는 변수에 저장함\n",
        "\n",
        "`answer = np.argmax(y_real,axis=1)`: 실제확률 중에서 가장 가능성이 높은 레벨을 선택하여 `answer`라는 변수에 저장함\n",
        "\n",
        "`correct = np.equal(estimate, answer)`: `estimate`과 `answer`를 비교하여, 값이 같으면 `True`를, 다른면 `False`를 `correct`라는 변수에 저장합니다.\n",
        "\n",
        "`return np.mean(correct)`: `correct`내의 `True`를 기준으로 평균을 외부로 출력합니다."
      ],
      "metadata": {
        "id": "hSS8Xu4F4D09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: backprop_postproc()"
      ],
      "metadata": {
        "id": "t64BUsdb2uJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Xv-4uAgGJkn5"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def backprop_postproc(aux_pp):\n",
        "    y_real, y_hat, entropy = aux_pp\n",
        "\n",
        "    g_loss_entropy = 1.0/np.prod(entropy.shape)\n",
        "    g_entropy_output = softmax_cross_entropy_with_derv(y_real,y_hat)\n",
        "\n",
        "    G_output = g_entropy_output * g_loss_entropy\n",
        "\n",
        "    return G_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`aux_pp`: 위에 `forward_postproc` 메소드에서 출력한 `[y_real, y_hat, entropy]` 값임\n",
        "\n",
        "`y_real, y_hat, entropy = aux_pp`: 리스트를 다시 각각 나눔.\n",
        "\n",
        "`g_loss_entropy = 1.0/np.prod(entropy.shape)`: cross_entropy loss 값을 normalization 함\n",
        "- 예측값에 맞추어 모델의 기울기를 조절해주기 위해 만든 변수라고 생각함.\n",
        "\n",
        "`g_entropy_output = softmax_cross_entropy_with_derv(y_real,y_hat)`: 위에 정의된 `softmax_cross_entropy_with_derv` 메소드를 사용하여 모델의 기울기를 `g_entropy_output`라는 변수에 저장함.\n",
        "\n",
        "`G_output = g_entropy_output * g_loss_entropy`: 모델의 예측에 대한 손실의 기울기를 계산함\n",
        "\n",
        "`return G_output`: 모델의 손실 기울기를 외부로 출력함.\n",
        "\n"
      ],
      "metadata": {
        "id": "uHpN3cOXHIM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Level5: softmax_cross_entropy_with_derv()"
      ],
      "metadata": {
        "id": "NcFDkZ01324G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "y5Lr3TOgJtxd"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax_cross_entropy_with_derv(y_real,y_hat):\n",
        "\n",
        "    return softmax(y_hat) - y_real"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`return softmax(y_hat) - y_real`: 예측값의 softmax 확율과 실제값의 차이로 기울기를 구해 외부로 출력함."
      ],
      "metadata": {
        "id": "y60UVQetFA_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Level6: softmax()"
      ],
      "metadata": {
        "id": "ndm6kN7pH3gl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x3FujRxH3gl"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax(x):\n",
        "\n",
        "    max_elem = np.max(x, axis = 1)\n",
        "\n",
        "    diff = (x.transpose() - max_elem).transpose()\n",
        "    exp = np.exp(diff)\n",
        "\n",
        "    sum_exp = np.sum(exp, axis = 1)\n",
        "\n",
        "    probs = (exp.transpose()/sum_exp).transpose()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Level5: softmax_cross_entropy()"
      ],
      "metadata": {
        "id": "JHqrDd9C3DrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ll3FbKAu3DrM"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax_cross_entropy(y_real, y_hat):\n",
        "\n",
        "    probs = softmax(y_hat)\n",
        "\n",
        "    #기존 확률값에 아주 작은 양수를 더해 log함수의 폭주 방지\n",
        "    return - np.sum(y_real * np.log(probs+1.0e-10), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`y_real`: test 확률\n",
        "\n",
        "`y_hat`: 예측확률\n",
        "\n",
        "\n",
        "`probs = softmax(y_hat)`예측확률을 softmax 확률로 변경하여 `probs`에 저장함\n",
        "\n",
        "`return - np.sum(y_real * np.log(probs+1.0e-10), axis=1)`: 각 행 (or 배치)에서 cross-entropy loss 값을 구함"
      ],
      "metadata": {
        "id": "IOBW2wM63DrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Level6: softmax()"
      ],
      "metadata": {
        "id": "O24mDLlq3DrN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "5icvpUte3DrO"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def softmax(x):\n",
        "\n",
        "    max_elem = np.max(x, axis = 1)\n",
        "\n",
        "    diff = (x.transpose() - max_elem).transpose()\n",
        "    exp = np.exp(diff)\n",
        "\n",
        "    sum_exp = np.sum(exp, axis = 1)\n",
        "\n",
        "    probs = (exp.transpose()/sum_exp).transpose()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@차지형\n",
        "\n",
        "**코드 설명:**\n",
        "\n",
        "`max_elem = np.max(x, axis = 1)`: 각 행에서 최대 값을 구하여, `max_elem`에 저장함\n",
        "\n",
        "`diff = (x.transpose() - max_elem).transpose()`: 각 행의 모든 값과 그 행의 차이를 구하여 `diff`에 저장함.\n",
        "- 아마 다름 `np.exp()`에서 negative 값을 positive 값으로 바꿀떄,값들 사이에 차이가 너무 벌어지지 않도록 차이를 구하서 거리를 좁히는 것 같음\n",
        "\n",
        "`exp = np.exp(diff)`: negative 값을 positve값으로 변경하여 `exp`로 저장함\n",
        "\n",
        "`sum_exp = np.sum(exp, axis = 1)`: 각 행에서 값들을 모두 더해서 `sum_exp`에 저장함\n",
        "\n",
        "`probs = (exp.transpose()/sum_exp).transpose()`: `exp`에서 'sum_exp'를 나누어 확률을 구함\n",
        "\n",
        "`return probs`: 확률을 function 외부로 배출함\n",
        "\n",
        "**추가**: pytorch를 사용할 경우, `transpose()`를 일일히 사용할 필요 없이, 내부에서 알아서 해줌."
      ],
      "metadata": {
        "id": "gwFLKEYT3DrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: backprop_neuralnet()"
      ],
      "metadata": {
        "id": "GUpbwEAv3K37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "\n",
        "def backprop_neuralnet(G_output, x):\n",
        "    # 갱신되는 값은 가중치 뿐 아니라 편향도 함께 이뤄집니다.\n",
        "\t# 그렇기에 이 두 값을 모두 전역변수로 지정하여 다음에 파라미터를 사용할 때는\n",
        "\t# 업데이트 된 파라미터를 사용할 수 있도록 합니다.\n",
        "\n",
        "    global weight, bias\n",
        "\n",
        "\t#가중치의 경사하강법 수식 구현\n",
        "\t# 입력 벡터의 전치행렬이 필요하므로 transpose() 메서드를 활용합니다.\n",
        "    x_transpose = x.transpose()\n",
        "\n",
        "\t# 전치된 입력 벡터(x_transpose)와 손실함수(L)를 순전파의 결괏값(Y)에 대하여 편미분한 결과(G_output)의 행렬곱셈을 수행합니다.\n",
        "    G_w = np.matmul(x_transpose, G_output)\n",
        "\n",
        "\n",
        "    '''편향의 경사하강법 수식 구현'''\n",
        "\t# 편향의 정리된 수식에 맞춰 행렬로 이뤄진 G_output 값을 np.sum()을 통해 연산처리 합니다.\n",
        "    G_b = np.sum(G_output, axis = 0)\n",
        "\n",
        "\t# 기존 가중치와 편향값을 업데이트하기 위한 대입 연산자를 활용하여 경사하강법 수식을 코드화 합니다.\n",
        "    weight -= LEARNING_RATE * G_w\n",
        "    bias   -= LEARNING_RATE * G_b"
      ],
      "metadata": {
        "id": "cP5Vu_Zg3Twk"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: run_test()"
      ],
      "metadata": {
        "id": "KddXC-M3yea9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def run_test(x, y):\n",
        "    # 1) 순전파의 결괏값인 예측값을 수행하는 메서드를 배치합니다.\n",
        "    # 여기서 언더바 '_' 의 의미는 해당 메서드에서 불필요하게 사용되어지는 변수기에\n",
        "    # 언더바 처리를 통해 숨겨주었습니다. (※ 협업 관계자와의 약속이 필요)\n",
        "    y_hat, _ = forward_neuralnet(x)\n",
        "\n",
        "    # 2) 순전파의 결괏값과 실제 결괏값을 활용하여 정확도를 구해주는 메서드를 배치합니다.\n",
        "    accuracy = eval_accuracy(y_hat, y)\n",
        "\n",
        "    # 정확도에 대하여 반환 처리를 수행합니다.\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "7e-_fYWoyoIN"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level4: forward_neuralnet()"
      ],
      "metadata": {
        "id": "SiylJikb4KHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: AnnModel1\n",
        "def forward_neuralnet(x):\n",
        "\n",
        "    y_hat = np.matmul(x, weight) + bias\n",
        "\n",
        "    return y_hat, x"
      ],
      "metadata": {
        "id": "vM3C4e9F4KHZ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Level5: eval_accuracy()"
      ],
      "metadata": {
        "id": "9eZ2weM_4QGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "G-KAfGdP4QGm"
      },
      "outputs": [],
      "source": [
        "# 메서드 출저: AnnModel3\n",
        "def eval_accuracy(y_hat,y_real):\n",
        "    estimate = np.argmax(y_hat,axis=1)\n",
        "    answer = np.argmax(y_real,axis=1)\n",
        "\n",
        "    correct = np.equal(estimate, answer)\n",
        "\n",
        "    return np.mean(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@ 차지형\n",
        "\n",
        "**설명**\n",
        "\n",
        "`estimate = np.argmax(y_hat,axis=1)`: 예측확률 중에서 가장 가능성이 높은 레벨을 선택하여 `estimate`라는 변수에 저장함\n",
        "\n",
        "`answer = np.argmax(y_real,axis=1)`: 실제확률 중에서 가장 가능성이 높은 레벨을 선택하여 `answer`라는 변수에 저장함\n",
        "\n",
        "`correct = np.equal(estimate, answer)`: `estimate`과 `answer`를 비교하여, 값이 같으면 `True`를, 다른면 `False`를 `correct`라는 변수에 저장합니다.\n",
        "\n",
        "`return np.mean(correct)`: `correct`내의 `True`를 기준으로 평균을 외부로 출력합니다."
      ],
      "metadata": {
        "id": "KzRf4rwN4QGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Level1: Set_hidden()"
      ],
      "metadata": {
        "id": "MEHBUVf86lDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "def set_hidden(info):\n",
        "    # hidden_cnt : 하나의 은닉계층인 경우 해당 은닉계층이 갖는 노드의 수\n",
        "\t# hidden_config : 다수의 은닉계층인 경우 해당 은닉계층들의 수와 노드의 수\n",
        "    global hidden_cnt, hidden_config\n",
        "\n",
        "    # 은닉계층 셋팅 과정에서 int 형식으로 들어오는 경우와 리스트 형식으로 들어오는 경우를 분리하였습니다.\n",
        "\t# int 형식은 하나의 은닉계층으로 고정\n",
        "    # list 형식은 하나 이상의 은닉계층\n",
        "    # isinstance() 는 첫 번째 매개변수 타입과 두 번째 매개변수 타입이 일치하는지 확인합니다.\n",
        "    if isinstance(info, int):\n",
        "        hidden_cnt = info      # int 형식이 맞는 경우 hidden_cnt 변수에 은닉 계층의 노드 수를 할당\n",
        "        hidden_config = None   # int 형식이 맞는 경우 hidden_config 변수는 None 처리\n",
        "\n",
        "\t# 사용자의 설정값이 list 형식인 경우 해당 info 에 담긴 값은 hidden_config 변수에 할당\n",
        "    else:\n",
        "        hidden_config = info"
      ],
      "metadata": {
        "id": "_X0xUqIK6qn1"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level2: init_param()"
      ],
      "metadata": {
        "id": "XV-C7P377423"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 기존에 파라미터를 초기화 하는 메서드의 재정의 입니다.\n",
        "def init_param():\n",
        "\n",
        "\t# 사용자가 정의한 은닉 계층의 정보에 따라 hidden_config 변수의 타입은 None 혹은 List로 지정됩니다.\n",
        "\t# hidden_config 변수의 타입이 None 이 아닌 경우 하나 이상의 은닉 계층을 갖으므로, 그에 따른 파라미터 초기화 메서드를 동작시킵니다.\n",
        "    if hidden_config is not None:\n",
        "\t\t# 사용자를 위한 은닉계층 안내 문구 출력 및 다수의 파라미터 생성을 위한 메서드 동작\n",
        "        print(f\"[안내] 은닉 계층 {len(hidden_config)}개를 갖는 다층 퍼셉트론이 적용됩니다.\")\n",
        "        init_param_hiddens() # 본 메서드는 곧 정의할 예정입니다.\n",
        "\n",
        "    # hidden_config 변수의 타입이 None 인 경우 하나의 은닉 계층을 갖으므로, 그에 따른 파라미터 초기화 메서드를 동작시킵니다.\n",
        "    else:\n",
        "\t\t# 사용자를 위한 은닉계층 안내 문구 출력 및 다수의 파라미터 생성을 위한 메서드 동작\n",
        "        print(\"[안내] 은닉 계층 하나를 갖는 다층 퍼셉트론이 적용됩니다.\")\n",
        "        init_param_hidden1() # 본 메서드는 곧 정의할 예정입니다."
      ],
      "metadata": {
        "id": "eHp_6zYB76FM"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Level3: init_param_hidden()"
      ],
      "metadata": {
        "id": "yTY9UtHI-Q4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 다수의 은닉계층이 있는 경우 각 계층 사이에 존재하는 파라미터 변수 생성 메서드\n",
        "def init_param_hiddens():\n",
        "    # input_cnt    : 입력계층의 노드 수\n",
        "    # output_cnt   : 출력계층의 노드 수\n",
        "    # hidden_config : 은닉계층에 대한 정보\n",
        "    # pm_hiddens: [입력계층]-> (파라미터{pm_hiddens}) ->[은닉계층]--> (파라미터{pm_hiddens}) -->[마지막 은닉계층]--> ...\n",
        "    # pm_output : [마지막 은닉계층]-> (파라미터{pm_output}) ->[출력계층]\n",
        "    global pm_output, pm_hiddens, input_cnt, output_cnt, hidden_config\n",
        "\n",
        "    # 입력계층부터 마지막 은닉계층 사이에 존재하는 파라미터 값을 저장하기 위한 빈리스트 사전 정의\n",
        "    # 마지막 은닉계층과 출력 계층 사이의 파라미터는 따로 변수를 둘 예정\n",
        "    pm_hiddens = []\n",
        "\n",
        "    # 반복문을 활용하여 파라미터를 생성할 예정이기에 input_cnt 변수의 값을\n",
        "    # prev_cnt 변수에 할당합니다.\n",
        "    prev_cnt = input_cnt\n",
        "\n",
        "    # hidden_config:list -> 은닉계층의 수와 폭 정보\n",
        "    # Ex) 6, [3,7], [3,6,9], ...\n",
        "    for hidden_cnt in hidden_config:\n",
        "\n",
        "        # 입력계층(prev_cnt)과 첫번째 은닉계층(hidden_cnt)의 파라미터 생성\n",
        "        # 해당 계층 사이의 파라미터는 pm_hiddens 변수에 쌓는다.\n",
        "        pm_hiddens.append(allocate_param_pair([prev_cnt, hidden_cnt]))\n",
        "\n",
        "        # 은닉 계층의 노드 수 정보를 prev_cnt 변수로 할당하여 allocate_param_pair()를 통해\n",
        "        # 다시 은닉 계층들 사이의 파라미터 변수를 생성할 수 있도록 합니다.\n",
        "        prev_cnt = hidden_cnt\n",
        "        # 모든 은닉계층들의 파라미터 생성을 마치면 반복문 탈출\n",
        "\n",
        "    # 마지막 은닉계층(prev_cnt)과 출력계층(output_cnt) 사이의 파라미터 생성 및 변수 생성\n",
        "    pm_output = allocate_param_pair([prev_cnt, output_cnt])"
      ],
      "metadata": {
        "id": "3qfMWfpG-R1m"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: allocate_param_pair()"
      ],
      "metadata": {
        "id": "RcfBkPEq-lv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 계층 사이의 파라미터 초기화를 진행하는 메서드\n",
        "# shape : 다음과 같은 리스트 형식으로 값을 전달받습니다.\n",
        "# ex) 독립변수 5개, 은닉계층의 노드 3개인 경우 [5,3]\n",
        "# ex) 은닉계층의 노드 수 3개, 출력계층의 노드 수 1개인 경우 [3,1]\n",
        "def allocate_param_pair(shape):\n",
        "    # [가중치 초기화 과정]\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD, shape)\n",
        "\n",
        "\t# [편향 초기화 과정]\n",
        "    # 편향은 리스트 형식으로 값을 전달받는 것을 고려하여 -1 인덱스를 활용\n",
        "    bias   = np.zeros(shape[-1])\n",
        "\n",
        "    # 파이썬의 데이터 타입 중 딕셔너리 구조로 정의합니다.\n",
        "\t# 가중치는 'w', 편향은 'b' 키로 접근하여 활용할 수 있도록 합니다.\n",
        "    return {'w':weight, 'b':bias}"
      ],
      "metadata": {
        "id": "sqnHvPHg-kzS"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level3: init_param_hidden1()"
      ],
      "metadata": {
        "id": "sjs5wvNB-GFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 하나의 은닉계층을 갖는 신경망의 파라미터변수 생성 메서드\n",
        "def init_param_hidden1():\n",
        "\n",
        "    # 추후 활용을 위한 전역변수 지정\n",
        "    global pm_output, pm_hidden, input_cnt, output_cnt, hidden_cnt\n",
        "    # input_cnt  : 사전 신경망 모델이 들어있는 파일에 정의된 입력 계층의 노드 수 (독립변수의 수)\n",
        "\t# output_cnt : 사전 신경망 모델이 들어있는 파일에 정의된 출력 계층의 노드 수 (종속변수의 수)\n",
        "    # pm_hidden : 입력계층과 은닉계층 사이에 존재하는 파라미터\n",
        "    # pm_output : 은닉계층과 출력계층 사이에 존재하는 파라미터\n",
        "\n",
        "\t# 사전에 정의한 allocate_param_pair() 메서드를 통해 계층과 계층사이의 파라미터가 담긴 변수를 생성합니다.\n",
        "    pm_hidden = allocate_param_pair([input_cnt, hidden_cnt])\n",
        "    pm_output = allocate_param_pair([hidden_cnt, output_cnt])"
      ],
      "metadata": {
        "id": "yD2YehaO-DKm"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: allocate_param_pair()"
      ],
      "metadata": {
        "id": "aCb26mQs-sgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 계층 사이의 파라미터 초기화를 진행하는 메서드\n",
        "# shape : 다음과 같은 리스트 형식으로 값을 전달받습니다.\n",
        "# ex) 독립변수 5개, 은닉계층의 노드 3개인 경우 [5,3]\n",
        "# ex) 은닉계층의 노드 수 3개, 출력계층의 노드 수 1개인 경우 [3,1]\n",
        "def allocate_param_pair(shape):\n",
        "    # [가중치 초기화 과정]\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD, shape)\n",
        "\n",
        "\t# [편향 초기화 과정]\n",
        "    # 편향은 리스트 형식으로 값을 전달받는 것을 고려하여 -1 인덱스를 활용\n",
        "    bias   = np.zeros(shape[-1])\n",
        "\n",
        "    # 파이썬의 데이터 타입 중 딕셔너리 구조로 정의합니다.\n",
        "\t# 가중치는 'w', 편향은 'b' 키로 접근하여 활용할 수 있도록 합니다.\n",
        "    return {'w':weight, 'b':bias}"
      ],
      "metadata": {
        "id": "XNVVy-wI-sgQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level2: forward_neuralnet(), backprop_neuralnet()"
      ],
      "metadata": {
        "id": "9q0oxWZa70I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 기존에 신경망의 선형연산을 수행하는 메서드를 재정의 합니다.\n",
        "# 선형연산에 필요한 독립변수가 담긴 변수를 매개변수로 취합니다.\n",
        "def forward_neuralnet(x):\n",
        "    # 사용자가 정의한 은닉 계층에 따라 hidden_config 변수의 타입은 None 혹은 List로 지정됩니다.\n",
        "    # hidden_config 변수의 타입이 None 이 아닌 경우 하나 이상의 은닉 계층을 갖으므로, 그에 따른 선형연산 메서드를 동작시킵니다.\n",
        "    if hidden_config is not None:\n",
        "        return forward_neuralnet_hiddens(x)\n",
        "\n",
        "    # hidden_config 변수의 타입이 None 인 경우 하나의 은닉 계층을 갖으므로, 그에 따른 선형연산 메서드를 동작시킵니다.\n",
        "    else:\n",
        "        return forward_neuralnet_hidden1(x)\n",
        "\n",
        "# 기존에 경사하강법에 따른 파라미터 업데이트 메서드를 재정의 합니다.\n",
        "# 위 방식과 마찬가지로 hidden_config 변수타입에 맞춰 동작하는 메서드를 달리합니다.\n",
        "# 매개변수는 경사하강법에 필요한 변수들을 취합니다.\n",
        "def backprop_neuralnet(G_output, hiddens):\n",
        "    if hidden_config is not None:\n",
        "        backprop_neuralnet_hiddens(G_output, hiddens)\n",
        "    else:\n",
        "        backprop_neuralnet_hidden1(G_output, hiddens)"
      ],
      "metadata": {
        "id": "8uKHZ9Ci9sge"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Level3:forward_neuralnet_hiddens()"
      ],
      "metadata": {
        "id": "pn5vjX2i-z-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 하나 이상의 은닉계층이 존재하는 경우 순전파 메서드 정의\n",
        "# x : 독립변수\n",
        "def forward_neuralnet_hiddens(x):\n",
        "    # pm_hiddens : 입력계층과 은닉계층들 사이의 파라미터(리스트, 딕셔너리)\n",
        "    # pm_output  : 마지막 은닉계층과 출력계층 사이의 파라미터(딕셔너리)\n",
        "    global pm_output, pm_hiddens\n",
        "\n",
        "    # 입력계층의 데이터(독립변수)\n",
        "    hidden = x    # 연산용\n",
        "    hiddens = [x] # 역전파 전달용 보조정보(기존값 포함)\n",
        "\n",
        "    # 반복문을 통해 은닉계층들의 파라미터 정보를 하나씩 전달받도록 함\n",
        "    # 입력계층부터 마지막 은닉계층까지의 연산을 진행\n",
        "    # 선형연산 결괏값 다음은 relu 적용\n",
        "    for pm_hidden in pm_hiddens:\n",
        "\n",
        "        hidden = relu(np.matmul(hidden, pm_hidden['w']) + pm_hidden['b'])\n",
        "        # 연산 결괏값의 할당\n",
        "        hiddens.append(hidden)\n",
        "\n",
        "    # 마지막 은닉계층까지의 연산결괏값과 출력계층의 파라미터의 연산\n",
        "    # 활성화 함수 적용 X\n",
        "    output = np.matmul(hidden, pm_output['w']) + pm_output['b']\n",
        "\n",
        "    # 연산 결괏값(output) 반환 및 경사하강법 적용을 위해 hiddens 반환\n",
        "    return output, hiddens"
      ],
      "metadata": {
        "id": "pvbBgjIs_FiH"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: relu()"
      ],
      "metadata": {
        "id": "XBGYezkM_uRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 활성화 함수 ReLU() 정의\n",
        "def relu(x):\n",
        "    # 음수와 0을 걸러주는 numpy 내부 메서드 활용\n",
        "    return np.maximum(x,0)"
      ],
      "metadata": {
        "id": "zs78gKoZ_uRg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Level3:forward_neuralnet_hiddens1()"
      ],
      "metadata": {
        "id": "rYLSE8CT-5HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 순전파를 수행하는 메서드 정의\n",
        "def forward_neuralnet_hidden1(x):\n",
        "\n",
        "    global pm_output, pm_hidden\n",
        "\n",
        "    # hidden : 은닉계층의 각 노드가 갖는 값\n",
        "    # 은닉계층의 가중치와 편향을 활용해 독립변수와의 연산 후 비선형 활성화 함수 relu() 적용\n",
        "    hidden = relu(np.matmul(x, pm_hidden['w']) + pm_hidden['b'])\n",
        "\n",
        "    # output : 출력계층의 각 노드가 갖는 값\n",
        "    # 출력 계층에 대한 선형 연산\n",
        "    output = np.matmul(hidden, pm_output['w']) + pm_output['b']\n",
        "\n",
        "    # 반환되어지는 값은 최종 연산 결괏값과 역전파에 활용되는 각 계층별 변수\n",
        "    return output, [x,hidden]"
      ],
      "metadata": {
        "id": "bsvBXd42_QbZ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: relu()"
      ],
      "metadata": {
        "id": "L2I05ELi_ULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 활성화 함수 ReLU() 정의\n",
        "def relu(x):\n",
        "    # 음수와 0을 걸러주는 numpy 내부 메서드 활용\n",
        "    return np.maximum(x,0)"
      ],
      "metadata": {
        "id": "lDWFgXtq_Zrg"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Level3:backprop_neuralnet_hiddens()"
      ],
      "metadata": {
        "id": "WvzUmGhz-61-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 파라미터 업데이트 매서드\n",
        "# G_output : 가장 마지막 계층의 ∂L/∂Y (delta_k)\n",
        "# aux : 독립변수, 은닉계층들이 갖는 노드 값\n",
        "def backprop_neuralnet_hiddens(G_output, aux):\n",
        "    global pm_output, pm_hiddens\n",
        "\n",
        "    # 독립변수와 은닉계층들의 노드값들을 hiddens 에 저장(리스트 타입)\n",
        "    hiddens = aux\n",
        "\n",
        "    # [출력계층과 마지막 은닉계층 사이의 파라미터 갱신 준비]\n",
        "    # 가중치 갱신에 필요한 수식 : ∂L/∂W (X^t * G)\n",
        "    # 가장 마지막 은닉 계층(hiddens[-1])의 행렬전환\n",
        "    g_output_w_out = hiddens[-1].transpose()\n",
        "    G_w_out = np.matmul(g_output_w_out, G_output)\n",
        "    # 편향 갱신에 필요한 수식 : ∂L/∂B (G)\n",
        "    G_b_out = np.sum(G_output, axis = 0)\n",
        "\n",
        "    # [마지막 은닉계층과 이전 은닉계층의 ∂L/∂Y (delta_k+1) 준비 - 1/2 단계]\n",
        "    # 업데이트가 되지 않은 가중치가 필요\n",
        "    g_output_hidden = pm_output['w'].transpose()\n",
        "    G_hidden = np.matmul(G_output, g_output_hidden)\n",
        "\n",
        "    # [출력계층과 마지막 은닉계층 사이의 파라미터 갱신]\n",
        "    # 출력계층과 마지막 은닉계층의 파라미터 업데이트\n",
        "    pm_output['w'] -= LEARNING_RATE * G_w_out\n",
        "    pm_output['b'] -= LEARNING_RATE * G_b_out\n",
        "\n",
        "    # 마지막 은닉 계층과 그 이전 계층들 사이의 파라미터 업데이트 과정\n",
        "    # 즉 뒤에서 부터 업데이트를 하기 위해 reversed() 를 활용\n",
        "    # reversed() : 주어진 값들을 거꾸로 반환\n",
        "    # pm_hiddens : 입력벡터 부터 마지막 은닉계층의 값을 담고 있음 (리스트)\n",
        "    for n in reversed(range(len(pm_hiddens))):\n",
        "\n",
        "        # [마지막 은닉계층과 이전 은닉계층의 ∂L/∂Y (delta_k+1) 준비 - 2/2단계]\n",
        "        # 2단계에서 구현하는 수식 : (𝛅_k * w_k) * 𝜑(h)\n",
        "        # 𝜑(h)에 매개변수는 가장 마지막 은닉계층의 값부터 순차적으로 들어와야함\n",
        "        G_hidden = G_hidden * relu_derv(hiddens[n+1])\n",
        "\n",
        "        # 가장 마지막 이전 계층의 파라미터 갱신 준비\n",
        "        g_hidden_w_hid = hiddens[n].transpose()\n",
        "        G_w_hid = np.matmul(g_hidden_w_hid, G_hidden)\n",
        "        G_b_hid = np.sum(G_hidden, axis = 0)\n",
        "\n",
        "        # 마지막 은닉계층의 이전 계층 ∂L/∂Y 연산 (𝛅_k * w_k)\n",
        "        g_hidden_hidden = pm_hiddens[n]['w'].transpose()\n",
        "        G_hidden = np.matmul(G_hidden, g_hidden_hidden)\n",
        "\n",
        "        # 마지막 은닉계층 이전 계층의 파라미터 갱신\n",
        "        pm_hiddens[n]['w'] -= LEARNING_RATE * G_w_hid\n",
        "        pm_hiddens[n]['b'] -= LEARNING_RATE * G_b_hid"
      ],
      "metadata": {
        "id": "GmY_C_2MAIMP"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: relu_derv()"
      ],
      "metadata": {
        "id": "mrlnVhw4AUNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 활성화 함수 ReLU 는 np.sign() 메서드로 구현할 수 있으며, 다음과 같은 특징을 갖습니다.\n",
        "# -1 if x < 0\n",
        "# 0 if x==0\n",
        "# 1 if x > 0\n",
        "def relu_derv(y):\n",
        "    return np.sign(y)"
      ],
      "metadata": {
        "id": "WFd9sfaOAV4R"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Level3:backprop_neuralnet_hiddens1()"
      ],
      "metadata": {
        "id": "Zw04xKgt-8xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 파라미터 갱신을 수행하는 메서드\n",
        "# G_output : 출력계층과 은닉계층 사이의 ∂L/∂Y\n",
        "# aux : 입력 벡터, 은닉 계층의 정보\n",
        "def backprop_neuralnet_hidden1(G_output, aux):\n",
        "\n",
        "    # pm_hidden : 입력계층과 은닉계층 사이의 파라미터\n",
        "    # pm_output : 은닉계층과 출력계층 사이의 파라미터\n",
        "    global pm_output, pm_hidden\n",
        "\n",
        "    # 입력 벡터, 은닉계층의 노드 값 각각 변수화\n",
        "    x, hidden = aux\n",
        "\n",
        "    # [출력계층과 은닉계층 사이의 파라미터 갱신 준비과정]\n",
        "    # 가중치 갱신 준비과정 : X^t * G\n",
        "    g_output_w_output = hidden.transpose()\n",
        "    G_w_out = np.matmul(g_output_w_output, G_output)\n",
        "    # 편향 갱신 준비과정 : G\n",
        "    G_b_out = np.sum(G_output, axis = 0)\n",
        "\n",
        "    # [G_hidden 생성과정 1단계]\n",
        "    # G_Hidden : 은닉계층과 입력계층 사이의 ∂L/∂Y\n",
        "    # 전체 수식 : (𝛅_k * w_k) * 𝜑(h)\n",
        "    # 1단계에서 구현하는 수식 : (𝛅_k * w_k)\n",
        "\n",
        "    # np.matmul() 연산 과정을 위해 행렬전환\n",
        "    # G_hidden 을 구하기 위해서는 업데이트가 되지 않은 가중치가 필요하기에\n",
        "    # 가중치 업데이트 전에 활용\n",
        "    g_output_hidden = pm_output['w'].transpose()\n",
        "    G_hidden = np.matmul(G_output, g_output_hidden)\n",
        "\n",
        "    # 출력계층과 은닉계층 사이의 파라미터 갱신\n",
        "    pm_output['w'] -= LEARNING_RATE * G_w_out\n",
        "    pm_output['b'] -= LEARNING_RATE * G_b_out\n",
        "\n",
        "\n",
        "    # [G_hidden 생성과정 2단계]\n",
        "    # G_Hidden : 은닉계층과 입력계층 사이의 ₩\n",
        "    # 전체 수식 : (𝛅_k * w_k) * 𝜑(h)\n",
        "    # 2단계에서 구현하는 수식 : (𝛅_k * w_k) * 𝜑(h)\n",
        "    # 은닉계층에서 비선형 활성화함수 relu가 사용되므로, 이에 따른 미분과정의 곱이 수행\n",
        "    G_hidden = G_hidden * relu_derv(hidden)\n",
        "\n",
        "    # 은닉계층과 입력계층 사이의 파라미터 갱신준비과정\n",
        "    # 가중치 갱신 준비과정\n",
        "    g_hidden_w_hid = x.transpose()\n",
        "    G_w_hid = np.matmul(g_hidden_w_hid, G_hidden)\n",
        "    # 편향 갱신 준비과정\n",
        "    G_b_hid = np.sum(G_hidden, axis=0)\n",
        "\n",
        "    # 은닉계층과 입력계층 사이의 파리미터 갱신\n",
        "    pm_hidden['w'] -= LEARNING_RATE * G_w_hid\n",
        "    pm_hidden['b'] -= LEARNING_RATE * G_b_hid"
      ],
      "metadata": {
        "id": "Akoblja3Aiv3"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Level4: relu_derv()"
      ],
      "metadata": {
        "id": "UXmSpQJOAncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메서드 출저: mlp\n",
        "\n",
        "# 활성화 함수 ReLU 는 np.sign() 메서드로 구현할 수 있으며, 다음과 같은 특징을 갖습니다.\n",
        "# -1 if x < 0\n",
        "# 0 if x==0\n",
        "# 1 if x > 0\n",
        "def relu_derv(y):\n",
        "    return np.sign(y)"
      ],
      "metadata": {
        "id": "VU5HL5WHAncC"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실행"
      ],
      "metadata": {
        "id": "5TrzvBCLArOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아무 값도 입력하지 않으면 은닉계층을 갖지 않는 SLP 이 구축됩니다.\n",
        "set_hidden([2,9])\n",
        "# 메서드 동작\n",
        "multiple_main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6A33u-NAtAm",
        "outputId": "a9d3c226-d666-42ec-ac02-f836b60bbd31"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[안내] 은닉 계층 2개를 갖는 다층 퍼셉트론이 적용됩니다.\n",
            "Epoch 1   : Train - Loss = 2.203, Accuracy = 0.341 / Test - Accuracy = 0.346\n",
            "Epoch 2   : Train - Loss = 1.838, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 3   : Train - Loss = 1.790, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 4   : Train - Loss = 1.758, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 5   : Train - Loss = 1.735, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 6   : Train - Loss = 1.718, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 7   : Train - Loss = 1.707, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 8   : Train - Loss = 1.698, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 9   : Train - Loss = 1.691, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "Epoch 10   : Train - Loss = 1.686, Accuracy = 0.347 / Test - Accuracy = 0.346\n",
            "==============================  Final TEST  ==============================\n",
            "\n",
            "Final Accuracy = 0.346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQKgnuZcCNOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Efj526P2u6aQ",
        "6Q23-QO-u90-",
        "1iV6UsPQvMU5",
        "IJmb6b3ix5iB",
        "4vKqXzInx8h1",
        "nSuSAKzeyQ0n",
        "tQozUXn3zDHh",
        "aeg9Wk491Utp",
        "w6OTKBXX2IAP",
        "2DeyHp0U2fjt",
        "NcFDkZ01324G",
        "SiylJikb4KHY",
        "9eZ2weM_4QGm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}